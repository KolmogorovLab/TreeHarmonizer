{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TreeHarmonizer\n",
    "\n",
    "This notebook serves to place SVs called by Severus onto the given phylogenetic tree.\n",
    "\n",
    "This notebook is being converted to serve as a element of a single TreeHarmonizer tool.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "----- Please consult the main project README.md for installation instructions -----\n",
    "\n",
    "TreeHarmonizer requires a Python 3.6 environment with the following packages -\n",
    "* pandas\n",
    "* io\n",
    "* functools \n",
    "* os\n",
    "* intervaltree (https://pypi.org/project/intervaltree/)\n",
    "* ete3\n",
    "* jupyter\n",
    "\n",
    "## How To Run\n",
    "\n",
    "1. Load Dependencies in the cell below\n",
    "2. Fill in input parameters per instructions\n",
    "3. Run all following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import TreeHarmonizer_utils as th_utils\n",
    "importlib.reload(th_utils)\n",
    "import subprocess\n",
    "\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option(\"display.expand_frame_repr\", True)\n",
    "pd.set_option(\"display.max_colwidth\", 1000)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Parameters\n",
    "\n",
    "### SV Path\n",
    "* Expects a direct path to the to the Severus output VCF of all somatic SVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify paths with your file structures below.\n",
    "# Paths are prepopulated for example data provided in the repository. \n",
    "# Relative paths get converted to absolute paths via os.path.abspath in the th_utils functions.\n",
    "\n",
    "sv_path = \"./example_data/sv/severus_chr1.vcf\"\n",
    "fn_rate = 0.15\n",
    "write_exclusive_vcfs = True\n",
    "write_cumulative_vcfs = True\n",
    "write_path_exclusive = \"./severus_output_vcfs/exclusive\"\n",
    "write_path_cumulative = \"./severus_output_vcfs/cumulative\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load severus and tree data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Severus data into a merged DataFrame\n",
    "severus = th_utils.generate_severus_df(severus_path=sv_path, simple_name=True)\n",
    "\n",
    "# Load tree input via newick string and parse it into various components\n",
    "imported_tree, non_terminals, terminals, non_terminal_paths, terminal_paths, non_terminal_leaves, terminal_paths_o_keys, non_terminal_paths_without_N1 = th_utils.get_tree_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out sex chromosomes and filter out second BND pair for no double counting the same SV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import severus data\n",
    "\n",
    "sev_df = pd.DataFrame()\n",
    "sev_df_no_vntr = pd.DataFrame()\n",
    "\n",
    "# Set CHROM column to type string\n",
    "severus['CHROM'] = severus['CHROM'].astype(str)\n",
    "\n",
    "sev = [severus]\n",
    "for x in range(len(sev)):\n",
    "    #print(\"Data type: \" + str(data_type) + \" | Data set: \" + str(x))\n",
    "\n",
    "    # Remove severus data with X or Y endpoints as sex chromosomes are not in use\n",
    "    data = sev[x]\n",
    "    sev_x_or_y_endpoint = data[ (data['ALT'].str.contains(\"X:\") == True) | (data['ALT'].str.contains(\"Y:\") == True)].copy()\n",
    "    ids_to_remove = sev_x_or_y_endpoint['ID'].tolist()\n",
    "    id_pairs_to_remove = [x[:-1]+\"2\" for x in ids_to_remove]\n",
    "    all_ids_to_remove = ids_to_remove + id_pairs_to_remove\n",
    "    data = data[~data['ID'].isin(all_ids_to_remove)].copy()\n",
    "    \n",
    "    # Remove second breakend pair of each BND type variant\n",
    "    bnd_second_breakends = data[ (data['ID'].str.contains(\"BND\") == True) & (data['ID'].str.endswith(\"2\") == True)].copy()\n",
    "    data = data[~data['ID'].isin(bnd_second_breakends['ID'])].copy()\n",
    "\n",
    "    #print(all_ids_to_remove)\n",
    "    sev[x] = data.copy(deep=True)\n",
    "\n",
    "sev_df = sev[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess common ancestors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def severus_called_sublines_helper(row):\n",
    "    # GT:VAF:hVAF:DR:DV\n",
    "    # A VCF BUG IN THE CURRENT VERSION IS CAUSING THE ORIGINAL FILTER TO BREAK\n",
    "    # return [col for col in severus.columns if col.startswith('C') and col != \"CHROM\" and row[col] != \"./.:0:0,0,0:0:0\"]\n",
    "    # FILTER HAS BEEN CHANGED TO THE FOLLOWING\n",
    "    output_subline_list = []\n",
    "    for col in severus.columns:\n",
    "        if col.startswith('C') and col != \"CHROM\":\n",
    "            internal_sev_data = row[col].split(\":\")\n",
    "            DV = int(internal_sev_data[4])\n",
    "            if DV > 0:\n",
    "                output_subline_list.append(col)\n",
    "    return output_subline_list\n",
    "\n",
    "\n",
    "sev_df['severus_called_sublines'] = sev_df.apply(lambda row: severus_called_sublines_helper(row), axis=1)\n",
    "sev_df['severus_mrca'] = sev_df.apply(lambda row: th_utils.common_ancestor_helper(row, \"severus_called_sublines\", imported_tree), axis=1)\n",
    "sev_df['severus_mrca_terminals'] = sev_df.apply(lambda row: imported_tree.search_nodes(name=row['severus_mrca'])[0].get_leaf_names(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine clade size acceptance requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prebuilt minimum subline support per clade size requirement\n",
    "# Uncomment below in order to use the prebuilt version rather than unnecessarily recalculating\n",
    "\n",
    "# minimum_subline_support_per_clade_size_requirement = {\n",
    "#     1: 1,\n",
    "#     2: 2,\n",
    "#     3: 2,\n",
    "#     4: 3,\n",
    "#     5: 4,\n",
    "#     7: 5,\n",
    "#     8: 6,\n",
    "#     12: 10,\n",
    "#     16: 13,\n",
    "#     23: 19\n",
    "# }\n",
    "\n",
    "# If other clade sizes are necessary, formulaic version of threshold is below, commented out.\n",
    "\n",
    "clade_sizes = set()\n",
    "# Get a list of all the clade sizes in the tree\n",
    "for clade in imported_tree.traverse():\n",
    "    if clade.is_leaf():\n",
    "        continue\n",
    "    clade_sizes.add(len(clade.get_leaves()))\n",
    "\n",
    "formulaic_subline_support_per_clade_size_requirement = {}\n",
    "\n",
    "cur_tree_clade_sizes = clade_sizes\n",
    "cur_tree_fn_rate = fn_rate # Default 15% as set in Cell 2, can be changed based on user preference\n",
    "for clade_size in cur_tree_clade_sizes:\n",
    "    if clade_size < 2:\n",
    "        formulaic_subline_support_per_clade_size_requirement[clade_size] = 1\n",
    "    elif clade_size == 2:\n",
    "        formulaic_subline_support_per_clade_size_requirement[clade_size] = 2\n",
    "    else:\n",
    "        # Calculate the support requirement based on the FN rate\n",
    "        support_requirement = int(clade_size * (1 - float(cur_tree_fn_rate)))\n",
    "        formulaic_subline_support_per_clade_size_requirement[clade_size] = support_requirement\n",
    "\n",
    "minimum_subline_support_per_clade_size_requirement = formulaic_subline_support_per_clade_size_requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine amount of placed variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     53\n",
      "False     1\n",
      "Name: minimum_subline_support_threshold_met_severus, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sev_df['called_subline_count'] = sev_df.apply(lambda row: len(row['severus_called_sublines']), axis=1)\n",
    "sev_df['terminal_count'] = sev_df.apply(lambda row: len(row['severus_mrca_terminals']), axis=1)\n",
    "sev_df['minimum_subline_support_threshold_met_severus'] = sev_df.apply(lambda row: row['called_subline_count'] >= minimum_subline_support_per_clade_size_requirement[row['terminal_count']], axis=1)\n",
    "\n",
    "print(sev_df['minimum_subline_support_threshold_met_severus'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Severus Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_severus_header = \"\"\"##fileformat=VCFv4.2\n",
    "##source=Severus_v1.3\n",
    "##CommandLine= --target-bam C1.bam C3.bam C4.bam C5.bam C6.bam C7.bam C8.bam C9.bam C10.bam C11.bam C12.bam C13.bam C14.bam C15.bam C16.bam C17.bam C18.bam C19.bam C20.bam C21.bam C22.bam C23.bam C24.bam --control-bam normal.bam --vntr-bed /data/keskusa2/Severus/vntrs/grcm38.trf.bed --out-dir /data/KolmogorovLab/keskusa2/nano_sv_calls/mouse_results/severus_pon_Mar11 -t 32 --resolve-overlaps --phasing-vcf /data/KolmogorovLab/agoretsky/margin_whatshap_output/output.phased.vcf.gz --low-quality --PON /data/KolmogorovLab/keskusa2/FVB_PON.tsv\n",
    "##fileDate=2025-03-13\n",
    "##contig=<ID=1,length=195471971>\n",
    "##contig=<ID=10,length=130694993>\n",
    "##contig=<ID=11,length=122082543>\n",
    "##contig=<ID=12,length=120129022>\n",
    "##contig=<ID=13,length=120421639>\n",
    "##contig=<ID=14,length=124902244>\n",
    "##contig=<ID=15,length=104043685>\n",
    "##contig=<ID=16,length=98207768>\n",
    "##contig=<ID=17,length=94987271>\n",
    "##contig=<ID=18,length=90702639>\n",
    "##contig=<ID=19,length=61431566>\n",
    "##contig=<ID=2,length=182113224>\n",
    "##contig=<ID=3,length=160039680>\n",
    "##contig=<ID=4,length=156508116>\n",
    "##contig=<ID=5,length=151834684>\n",
    "##contig=<ID=6,length=149736546>\n",
    "##contig=<ID=7,length=145441459>\n",
    "##contig=<ID=8,length=129401213>\n",
    "##contig=<ID=9,length=124595110>\n",
    "##contig=<ID=MT,length=16299>\n",
    "##contig=<ID=X,length=171031299>\n",
    "##contig=<ID=Y,length=91744698>\n",
    "##contig=<ID=JH584299.1,length=953012>\n",
    "##contig=<ID=GL456233.1,length=336933>\n",
    "##contig=<ID=JH584301.1,length=259875>\n",
    "##contig=<ID=GL456211.1,length=241735>\n",
    "##contig=<ID=GL456350.1,length=227966>\n",
    "##contig=<ID=JH584293.1,length=207968>\n",
    "##contig=<ID=GL456221.1,length=206961>\n",
    "##contig=<ID=JH584297.1,length=205776>\n",
    "##contig=<ID=JH584296.1,length=199368>\n",
    "##contig=<ID=GL456354.1,length=195993>\n",
    "##contig=<ID=JH584294.1,length=191905>\n",
    "##contig=<ID=JH584298.1,length=184189>\n",
    "##contig=<ID=JH584300.1,length=182347>\n",
    "##contig=<ID=GL456219.1,length=175968>\n",
    "##contig=<ID=GL456210.1,length=169725>\n",
    "##contig=<ID=JH584303.1,length=158099>\n",
    "##contig=<ID=JH584302.1,length=155838>\n",
    "##contig=<ID=GL456212.1,length=153618>\n",
    "##contig=<ID=JH584304.1,length=114452>\n",
    "##contig=<ID=GL456379.1,length=72385>\n",
    "##contig=<ID=GL456216.1,length=66673>\n",
    "##contig=<ID=GL456393.1,length=55711>\n",
    "##contig=<ID=GL456366.1,length=47073>\n",
    "##contig=<ID=GL456367.1,length=42057>\n",
    "##contig=<ID=GL456239.1,length=40056>\n",
    "##contig=<ID=GL456213.1,length=39340>\n",
    "##contig=<ID=GL456383.1,length=38659>\n",
    "##contig=<ID=GL456385.1,length=35240>\n",
    "##contig=<ID=GL456360.1,length=31704>\n",
    "##contig=<ID=GL456378.1,length=31602>\n",
    "##contig=<ID=GL456389.1,length=28772>\n",
    "##contig=<ID=GL456372.1,length=28664>\n",
    "##contig=<ID=GL456370.1,length=26764>\n",
    "##contig=<ID=GL456381.1,length=25871>\n",
    "##contig=<ID=GL456387.1,length=24685>\n",
    "##contig=<ID=GL456390.1,length=24668>\n",
    "##contig=<ID=GL456394.1,length=24323>\n",
    "##contig=<ID=GL456392.1,length=23629>\n",
    "##contig=<ID=GL456382.1,length=23158>\n",
    "##contig=<ID=GL456359.1,length=22974>\n",
    "##contig=<ID=GL456396.1,length=21240>\n",
    "##contig=<ID=GL456368.1,length=20208>\n",
    "##contig=<ID=JH584292.1,length=14945>\n",
    "##contig=<ID=JH584295.1,length=1976>\n",
    "##ALT=<ID=DEL,Description=\"Deletion\">\n",
    "##ALT=<ID=INS,Description=\"Insertion\">\n",
    "##ALT=<ID=DUP,Description=\"Duplication\">\n",
    "##ALT=<ID=INV,Description=\"Reciprocal Inversion\">\n",
    "##ALT=<ID=BND,Description=\"Breakend\">\n",
    "##FILTER=<ID=PASS,Description=\"All filters passed\">\n",
    "##FILTER=<ID=FAIL_LOWSUPP,Description=\"Less number of support, but ok in other samples\">\n",
    "##FILTER=<ID=FAIL_MAP_CONS,Description=\"Majority of variant reads have unreliable mappability\">\n",
    "##FILTER=<ID=FAIL_CONN_CONS,Description=\"Majority of variant reads have unreliable connections\">\n",
    "##FILTER=<ID=FAIL_LOWCOV_OTHER,Description=\"Low variant coverage in other samples\">\n",
    "##INFO=<ID=PRECISE,Number=0,Type=Flag,Description=\"SV with precise breakpoints coordinates and length\">\n",
    "##INFO=<ID=IMPRECISE,Number=0,Type=Flag,Description=\"SV with imprecise breakpoints coordinates and length\">\n",
    "##INFO=<ID=SVTYPE,Number=1,Type=String,Description=\"Type of structural variant\">\n",
    "##INFO=<ID=SVLEN,Number=1,Type=Integer,Description=\"Length of the SV\">\n",
    "##INFO=<ID=END,Number=1,Type=Integer,Description=\"End position of the SV\">\n",
    "##INFO=<ID=STRANDS,Number=1,Type=String,Description=\"Breakpoint strandedness\">\n",
    "##INFO=<ID=DETAILED_TYPE,Number=1,Type=String,Description=\"Detailed type of the SV\">\n",
    "##INFO=<ID=INSLEN,Number=1,Type=Integer,Description=\"Length of the unmapped sequence between breakpoint\">\n",
    "##INFO=<ID=MAPQ,Number=1,Type=Integer,Description=\"Median mapping quality of supporting reads\">\n",
    "##INFO=<ID=PHASESETID,Number=1,Type=String,Description=\"Matching phaseset ID for phased SVs\">\n",
    "##INFO=<ID=HP,Number=1,Type=Integer,Description=\"Matching haplotype ID for phased SVs\">\n",
    "##INFO=<ID=CLUSTERID,Number=1,Type=String,Description=\"Cluster ID in breakpoint_graph\">\n",
    "##INFO=<ID=INSSEQ,Number=1,Type=String,Description=\"Insertion sequence between breakpoints\">\n",
    "##INFO=<ID=MATE_ID,Number=1,Type=String,Description=\"MATE ID for breakends\">\n",
    "##INFO=<ID=INSIDE_VNTR,Number=1,Type=String,Description=\"True if an indel is inside a VNTR\">\n",
    "##INFO=<ID=ALIGNED_POS,Number=1,Type=String,Description=\"Position in the reference\">\n",
    "##INFO=<ID=LOW_COV_IN,Number=1,Type=String,Description=\"Samples that has low coverage in that region\">\n",
    "##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n",
    "##FORMAT=<ID=DR,Number=1,Type=Integer,Description=\"Number of reference reads\">\n",
    "##FORMAT=<ID=DV,Number=1,Type=Integer,Description=\"Number of variant reads\">\n",
    "##FORMAT=<ID=VAF,Number=1,Type=Float,Description=\"Variant allele frequency\">\n",
    "##FORMAT=<ID=hVAF,Number=3,Type=Float,Description=\"Haplotype specific variant Allele frequency (H0,H1,H2)\">\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate exclusive variant placement VCFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_exclusive_presence_absence_vcf(internal_node, input_merged_df):\n",
    "    df_filtered = input_merged_df[ (input_merged_df['minimum_subline_support_threshold_met_severus'] == True) & (input_merged_df['severus_mrca'] == internal_node) ]\n",
    "    return df_filtered.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Severus Count for node: N1 5\n",
      "Severus Count for node: N8 0\n",
      "Severus Count for node: N2 0\n",
      "Severus Count for node: N12 0\n",
      "Severus Count for node: N9 0\n",
      "Severus Count for node: N4 0\n",
      "Severus Count for node: N3 0\n",
      "Severus Count for node: N16 0\n",
      "Severus Count for node: N13 0\n",
      "Severus Count for node: N10 0\n",
      "Severus Count for node: O5 3\n",
      "Severus Count for node: O23 4\n",
      "Severus Count for node: N5 0\n",
      "Severus Count for node: O19 4\n",
      "Severus Count for node: O17 1\n",
      "Severus Count for node: N17 0\n",
      "Severus Count for node: O13 2\n",
      "Severus Count for node: N14 0\n",
      "Severus Count for node: O9 1\n",
      "Severus Count for node: N11 0\n",
      "Severus Count for node: O4 1\n",
      "Severus Count for node: N6 0\n",
      "Severus Count for node: N7 1\n",
      "Severus Count for node: N19 0\n",
      "Severus Count for node: N18 1\n",
      "Severus Count for node: N15 0\n",
      "Severus Count for node: O24 0\n",
      "Severus Count for node: O1 1\n",
      "Severus Count for node: O22 2\n",
      "Severus Count for node: O10 0\n",
      "Severus Count for node: O12 1\n",
      "Severus Count for node: O3 2\n",
      "Severus Count for node: O14 2\n",
      "Severus Count for node: N20 0\n",
      "Severus Count for node: O11 3\n",
      "Severus Count for node: O18 1\n",
      "Severus Count for node: O15 1\n",
      "Severus Count for node: O21 0\n",
      "Severus Count for node: O6 7\n",
      "Severus Count for node: N21 0\n",
      "Severus Count for node: O16 0\n",
      "Severus Count for node: N22 1\n",
      "Severus Count for node: O8 8\n",
      "Severus Count for node: O20 0\n",
      "Severus Count for node: O7 1\n"
     ]
    }
   ],
   "source": [
    "exclusive_dfs = {}\n",
    "\n",
    "# Generate Exclusive Presence Absence VCFs for every node\n",
    "for key, value in non_terminal_leaves.items():\n",
    "    framework_df_exclusive = generate_exclusive_presence_absence_vcf(key, sev_df)\n",
    "    reordered_df = framework_df_exclusive[['CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO', 'FORMAT', 'C1', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'C22', 'C23', 'C24']]\n",
    "    print(\"Severus Count for node: \" + str(key) + \" \" + str(len(framework_df_exclusive)))\n",
    "\n",
    "    if write_exclusive_vcfs:\n",
    "        path_prefix = write_path_exclusive.strip(\"/\")\n",
    "        subprocess.run(['mkdir', '-p', path_prefix])\n",
    "        th_utils.write_vcf(reordered_df, f\"{path_prefix}/{key}.vcf\", default_severus_header)\n",
    "\n",
    "    exclusive_dfs.update({key: framework_df_exclusive})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate cumulative variant placement VCFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Severus Count for node: N1 5\n",
      "Severus Count for node: N8 5\n",
      "Severus Count for node: N2 5\n",
      "Severus Count for node: N12 5\n",
      "Severus Count for node: N9 5\n",
      "Severus Count for node: N4 5\n",
      "Severus Count for node: N3 5\n",
      "Severus Count for node: N16 5\n",
      "Severus Count for node: N13 5\n",
      "Severus Count for node: N10 5\n",
      "Severus Count for node: O5 8\n",
      "Severus Count for node: O23 9\n",
      "Severus Count for node: N5 5\n",
      "Severus Count for node: O19 9\n",
      "Severus Count for node: O17 6\n",
      "Severus Count for node: N17 5\n",
      "Severus Count for node: O13 7\n",
      "Severus Count for node: N14 5\n",
      "Severus Count for node: O9 6\n",
      "Severus Count for node: N11 5\n",
      "Severus Count for node: O4 6\n",
      "Severus Count for node: N6 5\n",
      "Severus Count for node: N7 6\n",
      "Severus Count for node: N19 5\n",
      "Severus Count for node: N18 6\n",
      "Severus Count for node: N15 5\n",
      "Severus Count for node: O24 5\n",
      "Severus Count for node: O1 6\n",
      "Severus Count for node: O22 7\n",
      "Severus Count for node: O10 5\n",
      "Severus Count for node: O12 6\n",
      "Severus Count for node: O3 8\n",
      "Severus Count for node: O14 8\n",
      "Severus Count for node: N20 5\n",
      "Severus Count for node: O11 8\n",
      "Severus Count for node: O18 7\n",
      "Severus Count for node: O15 7\n",
      "Severus Count for node: O21 5\n",
      "Severus Count for node: O6 12\n",
      "Severus Count for node: N21 5\n",
      "Severus Count for node: O16 5\n",
      "Severus Count for node: N22 6\n",
      "Severus Count for node: O8 13\n",
      "Severus Count for node: O20 6\n",
      "Severus Count for node: O7 7\n"
     ]
    }
   ],
   "source": [
    "cumulative_severus_dfs = {}\n",
    "\n",
    "for key, value in non_terminal_paths.items():\n",
    "    merged_for_key = pd.concat([exclusive_dfs[x] for x in value], ignore_index=True)\n",
    "    \n",
    "    cumulative_severus_dfs.update({key: merged_for_key})\n",
    "    print(\"Severus Count for node: \" + str(key) + \" \" + str(len(merged_for_key)))\n",
    "\n",
    "    if write_cumulative_vcfs:\n",
    "        path_prefix = write_path_cumulative.strip(\"/\")\n",
    "        subprocess.run(['mkdir', '-p', path_prefix])\n",
    "        th_utils.write_vcf(merged_for_key, f\"{path_prefix}/{key}.vcf\", default_severus_header)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tree_ete3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
