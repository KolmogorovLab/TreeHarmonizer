{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TreeHarmonizer\n",
    "\n",
    "TreeHarmonizer is a utility that is used to place called variants onto a pre-existing phylogenetic tree, allowing for visualization of variant trajectories and evolutionary progression. TreeHarmonizer was developed with single nucleotide (SNV), structural (SV), and copy number (CNA) variants in mind, allowing for placement of each variant type.\n",
    "\n",
    "As of version 0.1, TreeHarmonizer works as a jupyter notebook, designed for the paper \"Long-read sequencing of single cell-derived melanoma subclones reveals divergent and parallel genomic and epigenomic evolutionary trajectories\", by Liu & Goretsky, et al. \n",
    "\n",
    "Preprint available on [biorxiv.](https://www.biorxiv.org/content/10.1101/2025.08.28.672865v1)\n",
    "Unaligned BAM files available on [NCBI SRA](https://www.ncbi.nlm.nih.gov/bioproject/1307171)\n",
    "Further project files available on [Zenodo](https://doi.org/10.5281/zenodo.16883901)\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "----- Please consult the main project README.md for installation instructions -----\n",
    "\n",
    "TreeHarmonizer requires a Python 3.6 environment with the following packages -\n",
    "* pandas\n",
    "* io\n",
    "* functools \n",
    "* os\n",
    "* intervaltree (https://pypi.org/project/intervaltree/)\n",
    "* ete3\n",
    "* jupyter\n",
    "\n",
    "## How To Run\n",
    "\n",
    "1. Load Dependencies in the cell below\n",
    "2. Fill in input parameters per instructions\n",
    "3. Run all following cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import TreeHarmonizer_utils as th_utils\n",
    "importlib.reload(th_utils)\n",
    "import subprocess\n",
    "\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option(\"display.expand_frame_repr\", True)\n",
    "pd.set_option(\"display.max_colwidth\", 1000)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Parameters\n",
    "\n",
    "### SNV Path\n",
    "* Expects folder structure within SNV path to be the following: `[snv_path]/[sample_name]/[sample_name].vcf`\n",
    "  * SNV vcfs are assumed to follow the standard VCF 4.2 format, output by DeepVariant.\n",
    "\n",
    "  * Sample name within the VCF file is assumed to be the same as `[sample_name]`.\n",
    "\n",
    "  * All folders within the path will be assumed to be a sample by default, except those that begin with an underscore `_`.\n",
    "\n",
    "  * In order to limit folders further, add the `predefined_sample_list` argument to the call to `th_utils.generate_merged_df` in the Load SNVs and Tree data cell.\n",
    "\n",
    "  * Argument value should be a list of directory names within the `snv_path`, VCF names are still expected to match the sample name.\n",
    "\n",
    "### SV Path\n",
    "* Expects a direct path to the to the Severus output VCF of all somatic SVs.\n",
    "\n",
    "### CNA Path\n",
    "* Follows the expected path structure of Wakhan output.\n",
    "* Expects folder structure within the CNA path to be the following: `[cna_path]/[sample_name]/bed_output/[sample]_copynumber_segments.bed`\n",
    "\n",
    "### FN rate\n",
    "* False negative rate expected as a float 0 < fn > 1. Default is 15%.\n",
    "\n",
    "### Tree Path (In development, unavailable)\n",
    "* Currently only expects the original tree for the paper.\n",
    "* Expects a phylogenetic tree in the standard newick format.\n",
    "\n",
    "### Newick Format (In development, unavailable)\n",
    "* Newick strings are read and processed by the ete3 package. Please choose the format number that represents the structure of your newick string.\n",
    "\n",
    "### Write VCFs\n",
    "* True or False, output VCFs per tree node.\n",
    "* False will result in only internal notebook variables being populated.\n",
    "\n",
    "### Write Path\n",
    "* Path for VCF output per tree node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify paths with your file structures below.\n",
    "# Paths are prepopulated for example data provided in the repository. \n",
    "# Relative paths get converted to absolute paths via os.path.abspath in the th_utils functions.\n",
    "\n",
    "snv_path = \"./example_data/snv/\"\n",
    "cna_path = \"./example_data/cna/\"\n",
    "sv_path = \"./example_data/sv/severus_chr1.vcf\"\n",
    "fn_rate = 0.15\n",
    "write_exclusive_vcfs = True\n",
    "write_cumulative_vcfs = True\n",
    "write_path_exclusive = \"./output_vcfs/exclusive\"\n",
    "write_path_cumulative = \"./output_vcfs/cumulative\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import SNV and Severus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SNV data into a merged DataFrame\n",
    "dv_merged, sample_list = th_utils.generate_merged_df(caller_path=snv_path)\n",
    "\n",
    "# Load Severus data into a merged DataFrame\n",
    "severus = th_utils.generate_severus_df(severus_path=sv_path, simple_name=True)\n",
    "\n",
    "# Load tree input via newick string and parse it into various components\n",
    "imported_tree, non_terminals, terminals, non_terminal_paths, terminal_paths, non_terminal_leaves, terminal_paths_o_keys, non_terminal_paths_without_N1 = th_utils.get_tree_data()\n",
    "\n",
    "# Add informative columns to the merged DataFrames that were lost from original merging.\n",
    "dv_merged['CHROM'] = dv_merged['KEY'].str.split(\":\").str[0]\n",
    "dv_merged['POS'] = dv_merged['KEY'].str.split(\":\").str[1]\n",
    "dv_merged['REF'] = dv_merged['KEY'].str.split(\":\").str[2]\n",
    "dv_merged['ALT'] = dv_merged['KEY'].str.split(\":\").str[3]\n",
    "\n",
    "severus['CHROM'] = severus['CHROM'].astype(str)\n",
    "severus['POS'] = severus['POS'].astype(int)\n",
    "dv_merged['CHROM'] = dv_merged['CHROM'].astype(str)\n",
    "dv_merged['POS'] = dv_merged['POS'].astype(int)\n",
    "\n",
    "# Rename INFO COLUMNS to avoid conflicts\n",
    "dv_merged.rename(columns={'INFO': 'DV_INFO'}, inplace=True)\n",
    "severus.rename(columns={'INFO': 'SEV_INFO'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Severus Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out second breakpoint pair\n",
    "severus_no_break = severus[severus[\"ID\"].str.contains(\"_2\")==False]\n",
    "severus_filtered = severus_no_break.copy(deep=True)\n",
    "\n",
    "# Filter for only deletions\n",
    "severus_filtered_del = severus_filtered[severus_filtered[\"SEV_INFO\"].str.contains(\"SVTYPE=DEL\")==True]\n",
    "\n",
    "# Create critical columns, note that these are NOT SETS, JUST ARRAYS (MRCA is a string)\n",
    "# Chain warning turned off, as we are not modifying the dataframe\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def severus_called_sublines_helper(row):\n",
    "    # GT:VAF:hVAF:DR:DV\n",
    "    # Prior filter method was:\n",
    "    # return [col for col in severus_data.columns if col.startswith('C') and col != \"CHROM\" and row[col] != \"./.:0:0,0,0:0:0\"]\n",
    "    # A VCF bug in the current Severus version (as of 03.11.2025) requires the following filter method to be used:\n",
    "    # FILTER HAS BEEN CHANGED TO THE FOLLOWING\n",
    "    output_subline_list = []\n",
    "    for col in severus_filtered_del.columns:\n",
    "        if col.startswith('C') and col != \"CHROM\":\n",
    "            internal_sev_data = row[col].split(\":\")\n",
    "            DV = int(internal_sev_data[4])\n",
    "            if DV > 0:\n",
    "                output_subline_list.append(col)\n",
    "    return output_subline_list\n",
    "\n",
    "severus_filtered_del['SEVERUS_SUBLINES'] = severus_filtered_del.apply(severus_called_sublines_helper, axis=1)\n",
    "severus_filtered_del['SEVERUS_MRCA'] = severus_filtered_del.apply(lambda row: th_utils.common_ancestor_helper(row, \"SEVERUS_SUBLINES\", input_tree=imported_tree), axis=1)\n",
    "severus_filtered_del['SEVERUS_MRCA_TERMINALS'] = severus_filtered_del.apply(lambda row: imported_tree.search_nodes(name=row['SEVERUS_MRCA'])[0].get_leaf_names(), axis=1)\n",
    "\n",
    "# Return chained assignment warning to default\n",
    "pd.options.mode.chained_assignment = 'warn'\n",
    "\n",
    "severus_filtered_del.index=severus_filtered_del['ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and process Wakhan CNA numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interval tree of all severus deletion ranges\n",
    "wakhan_cna_trees_per_chromosome = {}\n",
    "wakhan_cna_1_only_trees_per_chromosome = {}\n",
    "wakhan_cna_0_only_trees_per_chromosome = {}\n",
    "\n",
    "# Create blank interval trees per chromosome\n",
    "for sub in sample_list:\n",
    "    for chrom in th_utils.autosomes:\n",
    "        wakhan_cna_trees_per_chromosome.update({sub + \"-\" + chrom: th_utils.it.IntervalTree()})\n",
    "        wakhan_cna_1_only_trees_per_chromosome.update({sub + \"-\" + chrom: th_utils.it.IntervalTree()})\n",
    "        wakhan_cna_0_only_trees_per_chromosome.update({sub + \"-\" + chrom: th_utils.it.IntervalTree()})\n",
    "\n",
    "\n",
    "for subline in sample_list:\n",
    "    wk_copy_num = th_utils.read_bed_updated(cna_path + \"/\" + str(subline) + \"/bed_output/\" + str(subline) + \"_copynumbers_segments.bed\")\n",
    "    wk_copy_num['chr'] = wk_copy_num['chr'].astype(str)\n",
    "\n",
    "    # Filter wakhan down to only autosomes\n",
    "    wk_copy_num = th_utils.keep_rows_by_values(wk_copy_num, 'chr', th_utils.autosomes)\n",
    "    wk_copy_num['copynumber_state'] = wk_copy_num['copynumber_state'].astype(int)\n",
    "\n",
    "    # For each copy num entry, add to its respective interval tree, with the interval being the copy num range, data being the copy num metadata\n",
    "    for index, row in wk_copy_num.iterrows():\n",
    "        wakhan_cna_trees_per_chromosome[str(subline) + \"-\" + str(row['chr'])].addi(int(row['start']), int(row['end']) + 1, (str(subline), row['copynumber_state'], row['coverage'], row['confidence'], row['svs_breakpoints_ids']))\n",
    "\n",
    "    for index, row in wk_copy_num.iterrows():\n",
    "        if row['copynumber_state'] == 1:\n",
    "            wakhan_cna_1_only_trees_per_chromosome[str(subline) + \"-\" + str(row['chr'])].addi(int(row['start']), int(row['end']) + 1, (str(subline), row['copynumber_state'], row['coverage'], row['confidence'], row['svs_breakpoints_ids']))\n",
    "    \n",
    "    for index, row in wk_copy_num.iterrows():\n",
    "        if row['copynumber_state'] == 0:\n",
    "            wakhan_cna_0_only_trees_per_chromosome[str(subline) + \"-\" + str(row['chr'])].addi(int(row['start']), int(row['end']) + 1, (str(subline), row['copynumber_state'], row['coverage'], row['confidence'], row['svs_breakpoints_ids']))\n",
    "\n",
    "# Create a copy of the merged dataframe to modify\n",
    "df_merged_copy_wakhan = dv_merged.copy(deep=True)\n",
    "\n",
    "def in_copy_num_of_1_helper(row):\n",
    "    for subline in sample_list:\n",
    "        if wakhan_cna_1_only_trees_per_chromosome[str(subline) + \"-\" + row['CHROM']].overlaps_point(int(row['POS'])):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def in_copy_num_of_0_helper(row):\n",
    "    for subline in sample_list:\n",
    "        if wakhan_cna_0_only_trees_per_chromosome[str(subline) + \"-\" + row['CHROM']].overlaps_point(int(row['POS'])):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def copy_num_1_sublines_helper(row):\n",
    "    output = []\n",
    "    for subline in sample_list:\n",
    "        if wakhan_cna_1_only_trees_per_chromosome[str(subline) + \"-\" + row['CHROM']].overlaps_point(int(row['POS'])):\n",
    "            output.append(str(subline))\n",
    "    return output\n",
    "\n",
    "def copy_num_0_sublines_helper(row):\n",
    "    output = []\n",
    "    for subline in sample_list:\n",
    "        if wakhan_cna_0_only_trees_per_chromosome[str(subline) + \"-\" + row['CHROM']].overlaps_point(int(row['POS'])):\n",
    "            output.append(str(subline))\n",
    "    return output\n",
    "\n",
    "def get_cn_meta_helper(row, subline_col, interval_tree_dict, input_tree):\n",
    "    metadata = {}\n",
    "    for subline in row[subline_col]:\n",
    "        chrom = row['CHROM']\n",
    "        pos = int(row['POS'])\n",
    "        intervals = interval_tree_dict[subline + \"-\" + chrom][pos]\n",
    "        metadata[subline] = intervals.pop().data\n",
    "    return metadata\n",
    "\n",
    "# PER CN Loss Type Column Creation (For debugging and closer analysis)\n",
    "df_merged_copy_wakhan['IN_CN_1'] = df_merged_copy_wakhan.apply(lambda row: in_copy_num_of_1_helper(row), axis=1)\n",
    "df_merged_copy_wakhan['IN_CN_0'] = df_merged_copy_wakhan.apply(lambda row: in_copy_num_of_0_helper(row), axis=1)\n",
    "\n",
    "df_merged_copy_wakhan['CN_1_SUBLINES'] = df_merged_copy_wakhan.apply(lambda row: copy_num_1_sublines_helper(row), axis=1)\n",
    "df_merged_copy_wakhan['CN_0_SUBLINES'] = df_merged_copy_wakhan.apply(lambda row: copy_num_0_sublines_helper(row), axis=1)\n",
    "\n",
    "df_merged_copy_wakhan['CN_1_MRCA'] = df_merged_copy_wakhan.apply(lambda row: th_utils.common_ancestor_helper(row, \"CN_1_SUBLINES\", input_tree=imported_tree) if row['IN_CN_1'] else float(\"nan\"), axis=1)\n",
    "df_merged_copy_wakhan['CN_0_MRCA'] = df_merged_copy_wakhan.apply(lambda row: th_utils.common_ancestor_helper(row, \"CN_0_SUBLINES\", input_tree=imported_tree) if row['IN_CN_0'] else float(\"nan\"), axis=1)\n",
    "\n",
    "df_merged_copy_wakhan['CN_1_MRCA_TERMINALS'] = df_merged_copy_wakhan.apply(lambda row: imported_tree.search_nodes(name=row['CN_1_MRCA'])[0].get_leaf_names() if row['IN_CN_1'] else float(\"nan\"), axis=1)\n",
    "df_merged_copy_wakhan['CN_0_MRCA_TERMINALS'] = df_merged_copy_wakhan.apply(lambda row: imported_tree.search_nodes(name=row['CN_0_MRCA'])[0].get_leaf_names() if row['IN_CN_0'] else float(\"nan\"), axis=1)\n",
    "\n",
    "# CN 1 and 0 Combined for final union regenotyping\n",
    "df_merged_copy_wakhan['IN_CN_1_0'] = df_merged_copy_wakhan.apply(lambda row: row['IN_CN_1'] and row['IN_CN_0'], axis=1)\n",
    "df_merged_copy_wakhan['CN_1_0_SUBLINES'] = df_merged_copy_wakhan.apply(lambda row: row['CN_1_SUBLINES'] + row['CN_0_SUBLINES'], axis=1)\n",
    "df_merged_copy_wakhan['CN_1_0_MRCA'] = df_merged_copy_wakhan.apply(lambda row: th_utils.common_ancestor_helper(row, \"CN_1_0_SUBLINES\", input_tree=imported_tree) if row['IN_CN_1_0'] else float(\"nan\"), axis=1)\n",
    "df_merged_copy_wakhan['CN_1_0_MRCA_TERMINALS'] = df_merged_copy_wakhan.apply(lambda row: imported_tree.search_nodes(name=row['CN_1_0_MRCA'])[0].get_leaf_names() if row['IN_CN_1_0'] else float(\"nan\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cna_to_merge = df_merged_copy_wakhan[['CHROM', 'POS', 'IN_CN_1_0', 'CN_1_0_SUBLINES', 'CN_1_0_MRCA_TERMINALS', 'IN_CN_1', 'IN_CN_0', 'CN_0_SUBLINES', 'CN_1_SUBLINES', 'CN_0_MRCA_TERMINALS', 'CN_1_MRCA_TERMINALS']].copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MRCA records and metadata, severus MRCA metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKED_dv = dv_merged.copy(deep=True)\n",
    "\n",
    "# Add Wakhan CNA data to MARKED_dv\n",
    "MARKED_dv = MARKED_dv.merge(cna_to_merge, on=['CHROM', 'POS'], how='left')\n",
    "\n",
    "MARKED_dv['DV_SUBLINES'] = MARKED_dv.apply(lambda row: [col for col in MARKED_dv.columns if col.startswith('C') and col != \"CHROM\" and not col.startswith(\"CN\") and pd.notna(row[col])], axis=1)\n",
    "MARKED_dv['DV_MRCA'] = MARKED_dv.apply(lambda row: th_utils.common_ancestor_helper(row, \"DV_SUBLINES\", input_tree=imported_tree), axis=1)\n",
    "MARKED_dv['DV_MRCA_TERMINALS'] = MARKED_dv.apply(lambda row: imported_tree.search_nodes(name=row['DV_MRCA'])[0].get_leaf_names(), axis=1)\n",
    "MARKED_dv['DV_SUBLINE_COUNT'] = MARKED_dv.apply(lambda row: len(row['DV_SUBLINES']), axis=1)\n",
    "\n",
    "# Create interval tree of all severus deletion ranges\n",
    "severus_internal_trees_per_chromosome = {}\n",
    "\n",
    "# Create blank interval trees per chromosome\n",
    "for chrom in th_utils.autosomes:\n",
    "    severus_internal_trees_per_chromosome.update({chrom: th_utils.it.IntervalTree()})\n",
    "\n",
    "# For each deletion, add to its respective interval tree, with the interval being the deletion range, data being the deletion metadata\n",
    "# Severus deletions, as of Mar 11 version, are (, ], aka start exclusive, end inclusive.\n",
    "# Therefore we add 1 to the start position to make it inclusive, and add 1 to the end position to make it inclusive (interval tree is inclusive on lower, exclusive on upper, this has been verified.)\n",
    "for index, row in severus_filtered_del.iterrows():\n",
    "    severus_internal_trees_per_chromosome[row['CHROM']].addi(int(row['POS'] + 1), int(row['SEV_INFO'].split(\";\")[3].split(\"=\")[1]) + 1, (row['ID'], row['SEVERUS_MRCA'], row['SEVERUS_SUBLINES']))\n",
    "\n",
    "# Create new column in df_merged which states if variant position is in a severus deletion\n",
    "MARKED_dv['IN_SEVERUS_DELETION'] = MARKED_dv.apply(lambda row: len(severus_internal_trees_per_chromosome[row['CHROM']][int(row['POS'])]) > 0, axis=1)\n",
    "MARKED_dv['SEVERUS_SUBLINES'] = MARKED_dv.apply(lambda row: severus_internal_trees_per_chromosome[row['CHROM']][int(row['POS'])].pop()[2][2] if row['IN_SEVERUS_DELETION'] else float(\"nan\"), axis=1)\n",
    "MARKED_dv['SEVERUS_MRCA'] = MARKED_dv.apply(lambda row: severus_internal_trees_per_chromosome[row['CHROM']][int(row['POS'])].pop()[2][1] if row['IN_SEVERUS_DELETION'] else float(\"nan\"), axis=1)\n",
    "MARKED_dv['SEVERUS_MRCA_TERMINALS'] = MARKED_dv.apply(lambda row: imported_tree.search_nodes(name=row['SEVERUS_MRCA'])[0].get_leaf_names() if row['IN_SEVERUS_DELETION'] else float(\"nan\"), axis=1)\n",
    "MARKED_dv['SEVERUS_ID'] = MARKED_dv.apply(lambda row: severus_internal_trees_per_chromosome[row['CHROM']][int(row['POS'])].pop()[2][0] if row['IN_SEVERUS_DELETION'] else float(\"nan\"), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply parisomony assumption due to lack of phasing data in many regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any of the severus deletion sublines called MATCH a subline an SNV called subline\n",
    "# We make the assumption that deletion occurred on the other haplotype, and therefore not relevant to that SNV.\n",
    "# If the following col is True, do not use said deletion for this SNV.\n",
    "\n",
    "MARKED_dv['SEVERUS_OTHER_HAPLO_BOOL'] = MARKED_dv.apply(lambda row: len(set(row['DV_SUBLINES']).intersection(set(row['SEVERUS_SUBLINES']))) > 0 if row['IN_SEVERUS_DELETION'] else float(\"nan\"), axis=1)\n",
    "MARKED_dv['SEVERUS_OTHER_HAPLO_BOOL'] = MARKED_dv['SEVERUS_OTHER_HAPLO_BOOL'].astype('boolean')\n",
    "MARKED_dv['CN_OTHER_HAPLO_BOOL'] = MARKED_dv.apply(lambda row: len(set(row['DV_SUBLINES']).intersection(set(row['CN_1_0_SUBLINES']))) > 0 if row['IN_CN_1_0'] else float(\"nan\"), axis=1)\n",
    "MARKED_dv['CN_OTHER_HAPLO_BOOL'] = MARKED_dv['CN_OTHER_HAPLO_BOOL'].astype('boolean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create columns that reflect effects of regenotyping with only Severus deletions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union of DV and Severus deletion sublines\n",
    "MARKED_dv['REGENO_COMBINED_SUBLINES_SEVERUS_ONLY'] = MARKED_dv.apply(lambda row: list(set(row['SEVERUS_SUBLINES']).union(set(row['DV_SUBLINES']))) if (row['IN_SEVERUS_DELETION'] == True and row['SEVERUS_OTHER_HAPLO_BOOL'] == False) else float(\"nan\"), axis=1)\n",
    "\n",
    "# Common ancestor of DV and Severus deletion sublines\n",
    "MARKED_dv['REGENO_MRCA_SEVERUS_ONLY'] = MARKED_dv.apply(lambda row: th_utils.common_ancestor_helper(row, 'REGENO_COMBINED_SUBLINES_SEVERUS_ONLY', input_tree=imported_tree) if (row['IN_SEVERUS_DELETION'] and row['SEVERUS_OTHER_HAPLO_BOOL'] == False) else float(\"nan\"), axis=1)\n",
    "\n",
    "# TRUE/FALSE if the common ancestor of DV and Severus deletion sublines differs from the DV common ancestor\n",
    "MARKED_dv['REGENO_MRCA_SEVERUS_ONLY_DIFFERS'] = MARKED_dv.apply(lambda row: row['REGENO_MRCA_SEVERUS_ONLY'] != row['DV_MRCA'] if (row['IN_SEVERUS_DELETION'] and (row['SEVERUS_OTHER_HAPLO_BOOL'] == False)) else float(\"nan\"), axis=1)\n",
    "MARKED_dv['REGENO_MRCA_SEVERUS_ONLY_DIFFERS'] = MARKED_dv['REGENO_MRCA_SEVERUS_ONLY_DIFFERS'].astype('boolean')\n",
    "\n",
    "# Terminal nodes for the CHANGED common ancestor when including sublines from Severus deletion\n",
    "MARKED_dv['REGENO_MRCA_SEVERUS_ONLY_TERMINALS'] = MARKED_dv.apply(lambda row: imported_tree.search_nodes(name=row['REGENO_MRCA_SEVERUS_ONLY'])[0].get_leaf_names() if row['IN_SEVERUS_DELETION'] and (row['SEVERUS_OTHER_HAPLO_BOOL'] == False)else float(\"nan\"), axis=1)\n",
    "\n",
    "# TRUE/FALSE if the union of sublines from DV and Severus deletion differs from the DV sublines (MAY NOT DIFFER, IF DOESN'T, VARIANT WAS STILL CALLED IN DEL POSITION, IMPLIES OTHER HAPLOTYPE FOR DEL)\n",
    "MARKED_dv['REGENO_SUBLINES_DIFFER_SEVERUS_ONLY'] = MARKED_dv.apply(lambda row: set(row['REGENO_COMBINED_SUBLINES_SEVERUS_ONLY']) != set(row['DV_SUBLINES']) if row['IN_SEVERUS_DELETION'] and (row['SEVERUS_OTHER_HAPLO_BOOL'] == False) else float(\"nan\"), axis=1)\n",
    "MARKED_dv['REGENO_SUBLINES_DIFFER_SEVERUS_ONLY'] = MARKED_dv['REGENO_SUBLINES_DIFFER_SEVERUS_ONLY'].astype('boolean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create columns that reflect effects of regenotyping with only Wakhan losses (CN=0 or CN=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union of DV and CN 1 sublines\n",
    "MARKED_dv['REGENO_COMBINED_SUBLINES_CN_1_0_ONLY'] = MARKED_dv.apply(lambda row: list(set(row['CN_1_0_SUBLINES']).union(set(row['DV_SUBLINES']))) if row['IN_CN_1_0'] and row['CN_OTHER_HAPLO_BOOL'] == False else float(\"nan\"), axis=1)\n",
    "\n",
    "# Common ancestor of DV and CN 1 sublines\n",
    "MARKED_dv['REGENO_MRCA_CN_1_0_ONLY'] = MARKED_dv.apply(lambda row: th_utils.common_ancestor_helper(row, 'REGENO_COMBINED_SUBLINES_CN_1_0_ONLY', input_tree=imported_tree) if row['IN_CN_1_0'] and row['CN_OTHER_HAPLO_BOOL'] == False else float(\"nan\"), axis=1)\n",
    "\n",
    "# TRUE/FALSE if the common ancestor of DV and CN 1 sublines differs from the DV common ancestor\n",
    "MARKED_dv['REGENO_MRCA_CN_1_0_ONLY_DIFFERS'] = MARKED_dv.apply(lambda row: row['REGENO_MRCA_CN_1_0_ONLY'] != row['DV_MRCA'] if row['IN_CN_1_0'] and row['CN_OTHER_HAPLO_BOOL'] == False else float(\"nan\"), axis=1)\n",
    "MARKED_dv['REGENO_MRCA_CN_1_0_ONLY_DIFFERS'] = MARKED_dv['REGENO_MRCA_CN_1_0_ONLY_DIFFERS'].astype('boolean')\n",
    "\n",
    "# Terminal nodes for the CHANGED common ancestor when including sublines from CN 1\n",
    "MARKED_dv['REGENO_MRCA_CN_1_0_ONLY_TERMINALS'] = MARKED_dv.apply(lambda row: imported_tree.search_nodes(name=row['REGENO_MRCA_CN_1_0_ONLY'])[0].get_leaf_names() if row['IN_CN_1_0'] and row['CN_OTHER_HAPLO_BOOL'] == False else float(\"nan\"), axis=1)\n",
    "\n",
    "# TRUE/FALSE if the union of sublines from DV and CN 1 differs from the DV sublines (MAY NOT DIFFER, IF DOESN'T, VARIANT WAS STILL CALLED IN DEL POSITION, IMPLIES OTHER HAPLOTYPE FOR DEL)\n",
    "MARKED_dv['REGENO_SUBLINES_DIFFER_CN_1_0_ONLY'] = MARKED_dv.apply(lambda row: set(row['REGENO_COMBINED_SUBLINES_CN_1_0_ONLY']) != set(row['DV_SUBLINES']) if row['IN_CN_1_0'] and row['CN_OTHER_HAPLO_BOOL'] == False else float(\"nan\"), axis=1)\n",
    "MARKED_dv['REGENO_SUBLINES_DIFFER_CN_1_0_ONLY'] = MARKED_dv['REGENO_SUBLINES_DIFFER_CN_1_0_ONLY'].astype('boolean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create columns that reflect effects of regenoptying using both Severus and Wakhan losses.\n",
    "\n",
    "* Additionally, create dramatic shift column relfecting if regenotyping would dramatically shift the MRCA up the tree, herein defined as a subline reinclusion rate of >= 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_sublines_union_helper(row):\n",
    "    dv_set = set(row['DV_SUBLINES'])\n",
    "    CN_set = set()\n",
    "    SEV_set = set()\n",
    "    if row['IN_CN_1_0']:\n",
    "        CN_set = set(row['CN_1_0_SUBLINES'])\n",
    "    if row['IN_SEVERUS_DELETION']:\n",
    "        SEV_set = set(row['SEVERUS_SUBLINES'])\n",
    "    if SEV_set or CN_set:\n",
    "        return list(dv_set.union(CN_set).union(SEV_set))\n",
    "    else:\n",
    "        return float(\"nan\")\n",
    "\n",
    "# Union of DV and CN 1 sublines\n",
    "MARKED_dv['REGENO_COMBINED_SUBLINES_UNION'] = MARKED_dv.apply(lambda row: combined_sublines_union_helper(row), axis=1)\n",
    "\n",
    "# Common ancestor of DV and CN 1 sublines\n",
    "MARKED_dv['REGENO_MRCA_UNION'] = MARKED_dv.apply(lambda row: th_utils.common_ancestor_helper(row, 'REGENO_COMBINED_SUBLINES_UNION', input_tree=imported_tree) if (row['IN_CN_1_0'] and row['CN_OTHER_HAPLO_BOOL'] == False) or (row['IN_SEVERUS_DELETION'] and row['SEVERUS_OTHER_HAPLO_BOOL'] == False) else float(\"nan\"), axis=1)\n",
    "\n",
    "# TRUE/FALSE if the common ancestor of DV and CN 1 sublines differs from the DV common ancestor\n",
    "MARKED_dv['REGENO_MRCA_UNION_DIFFERS'] = MARKED_dv.apply(lambda row: row['REGENO_MRCA_UNION'] != row['DV_MRCA'] if (row['IN_CN_1_0'] and row['CN_OTHER_HAPLO_BOOL'] == False) or (row['IN_SEVERUS_DELETION'] and row['SEVERUS_OTHER_HAPLO_BOOL'] == False) else float(\"nan\"), axis=1)\n",
    "MARKED_dv['REGENO_MRCA_UNION_DIFFERS'] = MARKED_dv['REGENO_MRCA_UNION_DIFFERS'].astype('boolean')\n",
    "\n",
    "# Terminal nodes for the CHANGED common ancestor when including sublines from CN 1\n",
    "MARKED_dv['REGENO_MRCA_UNION_TERMINALS'] = MARKED_dv.apply(lambda row: imported_tree.search_nodes(name=row['REGENO_MRCA_UNION'])[0].get_leaf_names() if (row['IN_CN_1_0'] and row['CN_OTHER_HAPLO_BOOL'] == False) or (row['IN_SEVERUS_DELETION'] and row['SEVERUS_OTHER_HAPLO_BOOL'] == False) else float(\"nan\"), axis=1)\n",
    "\n",
    "# TRUE/FALSE if the union of sublines from DV and CN 1 differs from the DV sublines (MAY NOT DIFFER, IF DOESN'T, VARIANT WAS STILL CALLED IN DEL POSITION, IMPLIES OTHER HAPLOTYPE FOR DEL)\n",
    "MARKED_dv['REGENO_SUBLINES_DIFFER_UNION'] = MARKED_dv.apply(lambda row: set(row['REGENO_COMBINED_SUBLINES_UNION']) != set(row['DV_SUBLINES']) if (row['IN_CN_1_0'] and row['CN_OTHER_HAPLO_BOOL'] == False) or (row['IN_SEVERUS_DELETION'] and row['SEVERUS_OTHER_HAPLO_BOOL'] == False) else float(\"nan\"), axis=1)\n",
    "\n",
    "def tree_shift_helper(row):\n",
    "    if len(row['REGENO_COMBINED_SUBLINES_UNION']) > (len(row['DV_SUBLINES']) * 2):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "\n",
    "MARKED_dv['DRAMATIC_SHIFT'] = MARKED_dv.apply(lambda row: tree_shift_helper(row) if (row['IN_CN_1_0'] and row['CN_OTHER_HAPLO_BOOL'] == False) or (row['IN_SEVERUS_DELETION'] and row['SEVERUS_OTHER_HAPLO_BOOL'] == False) else float(\"nan\"), axis=1)\n",
    "MARKED_dv['DRAMATIC_SHIFT'] = MARKED_dv['DRAMATIC_SHIFT'].astype('boolean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the minimum subline support per clade size thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prebuilt minimum subline support per clade size requirement\n",
    "# Uncomment below in order to use the prebuilt version rather than unnecessarily recalculating\n",
    "\n",
    "# minimum_subline_support_per_clade_size_requirement = {\n",
    "#     1: 1,\n",
    "#     2: 2,\n",
    "#     3: 2,\n",
    "#     4: 3,\n",
    "#     5: 4,\n",
    "#     7: 5,\n",
    "#     8: 6,\n",
    "#     12: 10,\n",
    "#     16: 13,\n",
    "#     23: 19\n",
    "# }\n",
    "\n",
    "# If other clade sizes are necessary, formulaic version of threshold is below, commented out.\n",
    "\n",
    "clade_sizes = set()\n",
    "# Get a list of all the clade sizes in the tree\n",
    "for clade in imported_tree.traverse():\n",
    "    if clade.is_leaf():\n",
    "        continue\n",
    "    clade_sizes.add(len(clade.get_leaves()))\n",
    "\n",
    "formulaic_subline_support_per_clade_size_requirement = {}\n",
    "\n",
    "cur_tree_clade_sizes = clade_sizes\n",
    "cur_tree_fn_rate = fn_rate # Default 15% as set in Cell 2, can be changed based on user preference\n",
    "for clade_size in cur_tree_clade_sizes:\n",
    "    if clade_size < 2:\n",
    "        formulaic_subline_support_per_clade_size_requirement[clade_size] = 1\n",
    "    elif clade_size == 2:\n",
    "        formulaic_subline_support_per_clade_size_requirement[clade_size] = 2\n",
    "    else:\n",
    "        # Calculate the support requirement based on the FN rate\n",
    "        support_requirement = int(clade_size * (1 - float(cur_tree_fn_rate)))\n",
    "        formulaic_subline_support_per_clade_size_requirement[clade_size] = support_requirement\n",
    "\n",
    "minimum_subline_support_per_clade_size_requirement = formulaic_subline_support_per_clade_size_requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support prior to regenotyping and post regenotyping calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Pass or Fail calculation\n",
    "MARKED_dv['DV_MINIMUM_SUPPORT_MET'] = MARKED_dv.apply(lambda row: len(row['DV_SUBLINES']) >= minimum_subline_support_per_clade_size_requirement[len(row['DV_MRCA_TERMINALS'])], axis=1)\n",
    "MARKED_dv['DV_MINIMUM_SUPPORT_MET'] = MARKED_dv['DV_MINIMUM_SUPPORT_MET'].astype('boolean')\n",
    "MARKED_dv['SEVERUS_MINIMUM_SUPPORT_MET'] = MARKED_dv.apply(lambda row: len(row['SEVERUS_SUBLINES']) >= minimum_subline_support_per_clade_size_requirement[len(row['SEVERUS_MRCA_TERMINALS'])] if row['IN_SEVERUS_DELETION'] else float(\"nan\"), axis=1)\n",
    "MARKED_dv['SEVERUS_MINIMUM_SUPPORT_MET'] = MARKED_dv['SEVERUS_MINIMUM_SUPPORT_MET'].astype('boolean')\n",
    "MARKED_dv['CN_1_0_MINIMUM_SUPPORT_MET'] = MARKED_dv.apply(lambda row: len(row['CN_1_0_SUBLINES']) >= minimum_subline_support_per_clade_size_requirement[len(row['CN_1_0_MRCA_TERMINALS'])] if row['IN_CN_1_0'] else float(\"nan\"), axis=1)\n",
    "MARKED_dv['CN_1_0_MINIMUM_SUPPORT_MET'] = MARKED_dv['CN_1_0_MINIMUM_SUPPORT_MET'].astype('boolean')\n",
    "\n",
    "# Post REGENO Pass or Fail calculation\n",
    "MARKED_dv['REGENO_SEVERUS_ONLY_MINIMUM_SUPPORT_MET'] = MARKED_dv.apply(lambda row: len(row['REGENO_COMBINED_SUBLINES_SEVERUS_ONLY']) >= minimum_subline_support_per_clade_size_requirement[len(row['REGENO_MRCA_SEVERUS_ONLY_TERMINALS'])] if row['IN_SEVERUS_DELETION'] and row['SEVERUS_OTHER_HAPLO_BOOL'] == False else float(\"nan\"), axis=1)\n",
    "MARKED_dv['REGENO_SEVERUS_ONLY_MINIMUM_SUPPORT_MET'] = MARKED_dv['REGENO_SEVERUS_ONLY_MINIMUM_SUPPORT_MET'].astype('boolean')\n",
    "MARKED_dv['REGENO_CN_1_0_ONLY_MINIMUM_SUPPORT_MET'] = MARKED_dv.apply(lambda row: len(row['REGENO_COMBINED_SUBLINES_CN_1_0_ONLY']) >= minimum_subline_support_per_clade_size_requirement[len(row['REGENO_MRCA_CN_1_0_ONLY_TERMINALS'])] if row['IN_CN_1_0'] and row['CN_OTHER_HAPLO_BOOL'] == False else float(\"nan\"), axis=1)\n",
    "MARKED_dv['REGENO_CN_1_0_ONLY_MINIMUM_SUPPORT_MET'] = MARKED_dv['REGENO_CN_1_0_ONLY_MINIMUM_SUPPORT_MET'].astype('boolean')\n",
    "MARKED_dv['REGENO_UNION_MINIMUM_SUPPORT_MET'] = MARKED_dv.apply(lambda row: len(row['REGENO_COMBINED_SUBLINES_UNION']) >= minimum_subline_support_per_clade_size_requirement[len(row['REGENO_MRCA_UNION_TERMINALS'])] if (row['IN_CN_1_0'] and row['CN_OTHER_HAPLO_BOOL'] == False) or (row['IN_SEVERUS_DELETION'] and row['SEVERUS_OTHER_HAPLO_BOOL'] == False) else float(\"nan\"), axis=1)\n",
    "MARKED_dv['REGENO_UNION_MINIMUM_SUPPORT_MET'] = MARKED_dv['REGENO_UNION_MINIMUM_SUPPORT_MET'].astype('boolean')\n",
    "\n",
    "# Do not allow dramatic clade shifting\n",
    "MARKED_dv['REGENO_UNION_MINIMUM_SUPPORT_MET_NO_DRAMATIC_SHIFT'] = MARKED_dv.apply(lambda row: len(row['REGENO_COMBINED_SUBLINES_UNION']) >= minimum_subline_support_per_clade_size_requirement[len(row['REGENO_MRCA_UNION_TERMINALS'])] if ((row['IN_CN_1_0'] and row['CN_OTHER_HAPLO_BOOL'] == False) or (row['IN_SEVERUS_DELETION'] and row['SEVERUS_OTHER_HAPLO_BOOL'] == False)) and row['DRAMATIC_SHIFT'] == False else float(\"nan\"), axis=1)\n",
    "MARKED_dv['REGENO_UNION_MINIMUM_SUPPORT_MET_NO_DRAMATIC_SHIFT'] = MARKED_dv['REGENO_UNION_MINIMUM_SUPPORT_MET_NO_DRAMATIC_SHIFT'].astype('boolean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regenotyping Category Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Option 0: severus ---\n",
      "Category 1 DV Count: 2\n",
      "Category 2 DV Count: 2\n",
      "Category 3 DV Count: 0\n",
      "Category 4 DV Count: 0\n",
      "Category 5 DV Count: 2\n",
      "Category 6 DV Count: 0\n",
      "Category 7 DV Count: 0\n",
      "Category 8 DV Count: 208\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Processing Option 1: cna ---\n",
      "Category 1 DV Count: 0\n",
      "Category 2 DV Count: 0\n",
      "Category 3 DV Count: 0\n",
      "Category 4 DV Count: 0\n",
      "Category 5 DV Count: 0\n",
      "Category 6 DV Count: 0\n",
      "Category 7 DV Count: 0\n",
      "Category 8 DV Count: 0\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Processing Option 2: both ---\n",
      "Category 1 DV Count: 2\n",
      "Category 2 DV Count: 2\n",
      "Category 3 DV Count: 0\n",
      "Category 4 DV Count: 0\n",
      "Category 5 DV Count: 2\n",
      "Category 6 DV Count: 0\n",
      "Category 7 DV Count: 0\n",
      "Category 8 DV Count: 208\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Processing Option 3: both_no_dramatic ---\n",
      "Category 1 DV Count: 2\n",
      "Category 2 DV Count: 2\n",
      "Category 3 DV Count: 0\n",
      "Category 4 DV Count: 0\n",
      "Category 5 DV Count: 2\n",
      "Category 6 DV Count: 0\n",
      "Category 7 DV Count: 0\n",
      "Category 8 DV Count: 208\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vf/users/KolmogorovLab/agoretsky/conda/envs/tree_ete3/lib/python3.6/site-packages/ipykernel_launcher.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Generate all possible regenotyping category scoring\n",
    "# Severus only, CNA only, union of both, and the one that is used - union of both with dramatic shifts disallowed\n",
    "\n",
    "#original_support_met = MARKED_dv['DV_MINIMUM_SUPPORT_MET'] == True\n",
    "#regeno_cna_only_support_met = MARKED_dv['REGENO_CN_1_0_ONLY_MINIMUM_SUPPORT_MET'] == True\n",
    "#mrca_change_cna_only = MARKED_dv['REGENO_MRCA_CN_1_0_ONLY_DIFFERS'] == True\n",
    "#regeno_sv_only_support_met = MARKED_dv['REGENO_SEVERUS_ONLY_MINIMUM_SUPPORT_MET'] == True\n",
    "#mrca_change_sv_only = MARKED_dv['REGENO_MRCA_SEVERUS_ONLY_DIFFERS'] == True\n",
    "#regeno_union_support_met = MARKED_dv['REGENO_UNION_MINIMUM_SUPPORT_MET'] == True\n",
    "#mrca_change_union = MARKED_dv['REGENO_MRCA_UNION_DIFFERS'] == True\n",
    "#regeno_union_no_dramatic_shift_support_met = MARKED_dv['REGENO_UNION_MINIMUM_SUPPORT_MET_NO_DRAMATIC_SHIFT'] == True  \n",
    "\n",
    "\n",
    "# Define function blocks to return the masks for each scenario\n",
    "def get_scenario_columns(scenario_name):\n",
    "    if scenario_name == 'severus':\n",
    "        regeno_col = 'REGENO_SEVERUS_ONLY_MINIMUM_SUPPORT_MET'\n",
    "        mrca_col = 'REGENO_MRCA_SEVERUS_ONLY_DIFFERS'\n",
    "    elif scenario_name == 'cna':\n",
    "        regeno_col = 'REGENO_CN_1_0_ONLY_MINIMUM_SUPPORT_MET'\n",
    "        mrca_col = 'REGENO_MRCA_CN_1_0_ONLY_DIFFERS'\n",
    "    elif scenario_name == 'both':\n",
    "        regeno_col = 'REGENO_UNION_MINIMUM_SUPPORT_MET'\n",
    "        mrca_col = 'REGENO_MRCA_UNION_DIFFERS'\n",
    "    elif scenario_name == 'both_no_dramatic':\n",
    "        regeno_col = 'REGENO_UNION_MINIMUM_SUPPORT_MET_NO_DRAMATIC_SHIFT'\n",
    "        mrca_col = 'REGENO_MRCA_UNION_DIFFERS'\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scenario: {scenario_name}\")\n",
    "    \n",
    "    return ('DV_MINIMUM_SUPPORT_MET', regeno_col, mrca_col)\n",
    "\n",
    "scenarios = {\n",
    "    0: 'severus',\n",
    "    1: 'cna',\n",
    "    2: 'both',\n",
    "    3: 'both_no_dramatic'\n",
    "}\n",
    "\n",
    "cat_dvs_no_dramatic = []\n",
    "\n",
    "for option_num, scenario_name in scenarios.items():\n",
    "    print(f\"--- Processing Option {option_num}: {scenario_name} ---\")\n",
    "\n",
    "    orig_col, regeno_col, mrca_col = get_scenario_columns(scenario_name)\n",
    "\n",
    "    # Here we maintain the creation of explicit masks rather than the direct boolean indexing, as this \n",
    "    # combats the problems of having NaN values in the boolean columns, which leads to issues.\n",
    "    \n",
    "    original_support_TRUE = (MARKED_dv[orig_col] == True)\n",
    "    original_support_FALSE = (MARKED_dv[orig_col] == False)\n",
    "\n",
    "    regeno_support_TRUE = (MARKED_dv[regeno_col] == True)\n",
    "    regeno_support_FALSE = (MARKED_dv[regeno_col] == False)\n",
    "\n",
    "    mrca_change_TRUE = (MARKED_dv[mrca_col] == True)\n",
    "    mrca_change_FALSE = (MARKED_dv[mrca_col] == False)\n",
    "    \n",
    "    cat1_dv = MARKED_dv[(original_support_TRUE)  & (regeno_support_TRUE)  & (mrca_change_FALSE)]\n",
    "    cat2_dv = MARKED_dv[(original_support_TRUE)  & (regeno_support_TRUE)  & (mrca_change_TRUE)]\n",
    "    cat3_dv = MARKED_dv[(original_support_FALSE) & (regeno_support_TRUE)  & (mrca_change_FALSE)]\n",
    "    cat4_dv = MARKED_dv[(original_support_FALSE) & (regeno_support_TRUE)  & (mrca_change_TRUE)]\n",
    "    cat5_dv = MARKED_dv[(original_support_FALSE) & (regeno_support_FALSE) & (mrca_change_FALSE)]\n",
    "    cat6_dv = MARKED_dv[(original_support_FALSE) & (regeno_support_FALSE) & (mrca_change_TRUE)]\n",
    "    cat7_dv = MARKED_dv[(original_support_TRUE)  & (regeno_support_FALSE) & (mrca_change_FALSE)]\n",
    "    cat8_dv = MARKED_dv[(original_support_TRUE)  & (regeno_support_FALSE) & (mrca_change_TRUE)]\n",
    "\n",
    "    cat_dvs = [cat1_dv, cat2_dv, cat3_dv, cat4_dv, cat5_dv, cat6_dv, cat7_dv, cat8_dv]\n",
    "\n",
    "    # Add column and save the cat_dvs for the no_dramatic option for later use\n",
    "    if option_num == 3:\n",
    "        for i, cat_df in enumerate(cat_dvs):\n",
    "            if not cat_df.empty:\n",
    "                cat_dvs[i]['ADDED_REGENO_SUBLINES'] = cat_dvs[i].apply(lambda row: set(row['REGENO_COMBINED_SUBLINES_UNION']) - set(row['DV_SUBLINES']), axis=1)\n",
    "        cat_dvs_no_dramatic = cat_dvs\n",
    "\n",
    "    for i, cat_df in enumerate(cat_dvs):\n",
    "        print(f\"Category {i+1} DV Count: {len(cat_df)}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconcile passing categories back to VCF, Union (SV and CNA), no dramatic change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category 1 is kept as is, no change to MRCA, and passes anyway\n",
    "FINAL_VAR_DF = cat_dvs_no_dramatic[0].copy(deep=True)\n",
    "\n",
    "# Category 2, Need to look through the details. \n",
    "FINAL_VAR_DF = pd.concat([FINAL_VAR_DF, cat_dvs_no_dramatic[1]], ignore_index=True)\n",
    "\n",
    "# Category 3, Regenotyped with an unchanged MRCA, include all.\n",
    "FINAL_VAR_DF = pd.concat([FINAL_VAR_DF, cat_dvs_no_dramatic[2]], ignore_index=True)\n",
    "\n",
    "# Category 4, Regenotyped with a changed MRCA from originally failing.\n",
    "FINAL_VAR_DF = pd.concat([FINAL_VAR_DF, cat_dvs_no_dramatic[3]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blanket Default VCF Header\n",
    "default_header = \"\"\"##fileformat=VCFv4.2\n",
    "##FILTER=<ID=PASS,Description=\"All filters passed\">\n",
    "##FILTER=<ID=RefCall,Description=\"Genotyping model thinks this site is reference.\">\n",
    "##FILTER=<ID=LowQual,Description=\"Confidence in this variant being real is below calling threshold.\">\n",
    "##FILTER=<ID=NoCall,Description=\"Site has depth=0 resulting in no call.\">\n",
    "##INFO=<ID=END,Number=1,Type=Integer,Description=\"End position (for use with symbolic alleles)\">\n",
    "##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n",
    "##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Conditional genotype quality\">\n",
    "##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read depth\">\n",
    "##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=\"Minimum DP observed within the GVCF block.\">\n",
    "##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\"Read depth for each allele\">\n",
    "##FORMAT=<ID=VAF,Number=A,Type=Float,Description=\"Variant allele fractions.\">\n",
    "##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\"Phred-scaled genotype likelihoods rounded to the closest integer\">\n",
    "##FORMAT=<ID=MED_DP,Number=1,Type=Integer,Description=\"Median DP observed within the GVCF block rounded to the nearest integer.\">\n",
    "##DeepVariant_version=1.6.0\n",
    "##contig=<ID=1,length=195471971>\n",
    "##contig=<ID=10,length=130694993>\n",
    "##contig=<ID=11,length=122082543>\n",
    "##contig=<ID=12,length=120129022>\n",
    "##contig=<ID=13,length=120421639>\n",
    "##contig=<ID=14,length=124902244>\n",
    "##contig=<ID=15,length=104043685>\n",
    "##contig=<ID=16,length=98207768>\n",
    "##contig=<ID=17,length=94987271>\n",
    "##contig=<ID=18,length=90702639>\n",
    "##contig=<ID=19,length=61431566>\n",
    "##contig=<ID=2,length=182113224>\n",
    "##contig=<ID=3,length=160039680>\n",
    "##contig=<ID=4,length=156508116>\n",
    "##contig=<ID=5,length=151834684>\n",
    "##contig=<ID=6,length=149736546>\n",
    "##contig=<ID=7,length=145441459>\n",
    "##contig=<ID=8,length=129401213>\n",
    "##contig=<ID=9,length=124595110>\\n\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add REGENO entry to variants that are regenotyped rather than called originally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_nodes = [f'N{i}' for i in range(1, 23)]\n",
    "private_nodes = [f'O{i}' for i in range(1, 25) if i != 2]\n",
    "\n",
    "for index, row in FINAL_VAR_DF.iterrows():\n",
    "    if 'ADDED_REGENO_SUBLINES' in row and isinstance(row['ADDED_REGENO_SUBLINES'], set):\n",
    "        for subline in row['ADDED_REGENO_SUBLINES']:\n",
    "            if subline in FINAL_VAR_DF.columns:\n",
    "                FINAL_VAR_DF.at[index, subline] = \"REGENO\"\n",
    "\n",
    "columns_to_keep = ['KEY', 'CHROM', 'POS', 'REF', 'ALT', 'REGENO_MRCA_UNION'] + [f'C{i}' for i in range(1, 25) if i != 2]\n",
    "FINAL_VAR_DF = FINAL_VAR_DF[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add back non-regenotyped variants to the final output dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36359\n",
      "36363\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Add back all of the non-regenotyped variants.\n",
    "dv_merged_copy = dv_merged.copy(deep=True)\n",
    "dv_merged_copy = dv_merged_copy[~dv_merged_copy['KEY'].isin(FINAL_VAR_DF['KEY'])]\n",
    "\n",
    "print(len(dv_merged_copy))\n",
    "print(len(dv_merged))\n",
    "print(len(FINAL_VAR_DF))\n",
    "\n",
    "FINAL_VAR_DF.index = FINAL_VAR_DF['KEY']\n",
    "FINAL_VAR_DF = FINAL_VAR_DF.drop(columns=['KEY'])\n",
    "dv_merged_copy.index = dv_merged_copy['KEY']\n",
    "dv_merged_copy = dv_merged_copy.drop(columns=['KEY'])\n",
    "\n",
    "dv_merged_copy = dv_merged_copy.append(FINAL_VAR_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata for FINAL_MRCA column which will show to which internal branch / node this variant is placed at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_merged_copy['DV_SUBLINES'] = dv_merged_copy.apply(lambda row: [col for col in dv_merged_copy.columns if col.startswith('C') and col != \"CHROM\" and not col.startswith(\"CN\") and pd.notna(row[col])], axis=1)\n",
    "dv_merged_copy['DV_MRCA'] = dv_merged_copy.apply(lambda row: th_utils.common_ancestor_helper(row, \"DV_SUBLINES\", input_tree=imported_tree), axis=1)\n",
    "dv_merged_copy['DV_MRCA_TERMINALS'] = dv_merged_copy.apply(lambda row: imported_tree.search_nodes(name=row['DV_MRCA'])[0].get_leaf_names(), axis=1)\n",
    "dv_merged_copy['DV_MINIMUM_SUPPORT_MET'] = dv_merged_copy.apply(lambda row: len(row['DV_SUBLINES']) >= minimum_subline_support_per_clade_size_requirement[len(row['DV_MRCA_TERMINALS'])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create FINAL_MRCA column - where variant is placed at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_mrca_helper(row):\n",
    "    if type(row['REGENO_MRCA_UNION']) == float and pd.isna(row['REGENO_MRCA_UNION']):\n",
    "        if row['DV_MINIMUM_SUPPORT_MET']:\n",
    "            return row['DV_MRCA']\n",
    "        else:\n",
    "            return float('nan')\n",
    "    else:\n",
    "        return row['REGENO_MRCA_UNION']\n",
    "\n",
    "dv_merged_copy['FINAL_MRCA'] = dv_merged_copy.apply(lambda row: final_mrca_helper(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show amount of unplaced variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaN in FINAL_MRCA: 458\n",
      "1.2595220416357287\n"
     ]
    }
   ],
   "source": [
    "nan_count = dv_merged_copy['FINAL_MRCA'].isna().sum()\n",
    "print(f\"Number of rows with NaN in FINAL_MRCA: {nan_count}\")\n",
    "\n",
    "print(nan_count / len(dv_merged_copy) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsection merged DF into sub_df per internal and private node for VCF creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_node_dfs = {}\n",
    "private_node_dfs = {}\n",
    "\n",
    "dv_merged_without_unplaced = dv_merged_copy.dropna(subset=['FINAL_MRCA'])\n",
    "\n",
    "for internal_node in internal_nodes:\n",
    "    internal_node_dfs[internal_node] = dv_merged_without_unplaced[dv_merged_without_unplaced['FINAL_MRCA'] == internal_node]\n",
    "\n",
    "for private_node in private_nodes:\n",
    "    private_node_dfs[private_node] = dv_merged_without_unplaced[dv_merged_without_unplaced['FINAL_MRCA'] == private_node]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare exclusive DF formatting for VCF creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vf/users/KolmogorovLab/agoretsky/conda/envs/tree_ete3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/vf/users/KolmogorovLab/agoretsky/conda/envs/tree_ete3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/vf/users/KolmogorovLab/agoretsky/conda/envs/tree_ete3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/vf/users/KolmogorovLab/agoretsky/conda/envs/tree_ete3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/vf/users/KolmogorovLab/agoretsky/conda/envs/tree_ete3/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/vf/users/KolmogorovLab/agoretsky/conda/envs/tree_ete3/lib/python3.6/site-packages/pandas/core/frame.py:4327: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n"
     ]
    }
   ],
   "source": [
    "exclusive_dfs = {**internal_node_dfs, **private_node_dfs}\n",
    "\n",
    "for node in exclusive_dfs:\n",
    "    exclusive_dfs[node] = exclusive_dfs[node][['CHROM', 'POS', 'REF', 'ALT'] + [col for col in exclusive_dfs[node].columns if col.startswith('C') and col != \"CHROM\"]]\n",
    "    exclusive_dfs[node]['ID'] = \".\"\n",
    "    exclusive_dfs[node]['QUAL'] = \".\"\n",
    "    exclusive_dfs[node]['FILTER'] = \"PASS\"\n",
    "    exclusive_dfs[node]['INFO'] = \".\"\n",
    "    exclusive_dfs[node]['FORMAT'] = \"GT:GQ:DP:MIN_DP:AD:VAF:PL:MED_DP\"\n",
    "    # Fill Empty cells with \"./.:0:0:0:0,0:0:0:0\"\n",
    "    # exclusive_dfs[node].fillna(\"./.:0:0:0:0,0:0:0:0\", inplace=True)\n",
    "\n",
    "    # Fill Empty cells with \"./.:0:0,0,0:0:0:\" (As of unphased Sev data)\n",
    "    exclusive_dfs[node].fillna(\"./.:0:0,0,0:0:0\", inplace=True)\n",
    "\n",
    "    # Reorder columns\n",
    "    exclusive_dfs[node] = exclusive_dfs[node][['CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO', 'FORMAT'] + [col for col in exclusive_dfs[node].columns if col.startswith('C') and col != \"CHROM\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Exclusive VCFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N1 :  2879\n",
      "N2 :  38\n",
      "N3 :  23\n",
      "N4 :  38\n",
      "N5 :  27\n",
      "N6 :  67\n",
      "N7 :  539\n",
      "N8 :  12\n",
      "N9 :  11\n",
      "N10 :  302\n",
      "N11 :  30\n",
      "N12 :  16\n",
      "N13 :  79\n",
      "N14 :  195\n",
      "N15 :  23\n",
      "N16 :  131\n",
      "N17 :  51\n",
      "N18 :  874\n",
      "N19 :  78\n",
      "N20 :  145\n",
      "N21 :  268\n",
      "N22 :  208\n",
      "O1 :  2077\n",
      "O3 :  1020\n",
      "O4 :  1625\n",
      "O5 :  1472\n",
      "O6 :  609\n",
      "O7 :  576\n",
      "O8 :  911\n",
      "O9 :  1700\n",
      "O10 :  1293\n",
      "O11 :  1583\n",
      "O12 :  1331\n",
      "O13 :  1487\n",
      "O14 :  983\n",
      "O15 :  528\n",
      "O16 :  2006\n",
      "O17 :  2145\n",
      "O18 :  619\n",
      "O19 :  2340\n",
      "O20 :  554\n",
      "O21 :  834\n",
      "O22 :  1909\n",
      "O23 :  1703\n",
      "O24 :  566\n"
     ]
    }
   ],
   "source": [
    "for node in exclusive_dfs:\n",
    "    print(node, \": \", len(exclusive_dfs[node]))\n",
    "    if write_exclusive_vcfs:\n",
    "        path_prefix = write_path_exclusive.strip(\"/\")\n",
    "        subprocess.run(['mkdir', '-p', path_prefix])\n",
    "        th_utils.write_vcf(exclusive_dfs[node], f\"{path_prefix}/{node}.vcf\", default_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Cumulative VCFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N1 :  2879\n",
      "N8 :  2891\n",
      "N2 :  2917\n",
      "N12 :  2907\n",
      "N9 :  2902\n",
      "N4 :  2955\n",
      "N3 :  2940\n",
      "N16 :  3038\n",
      "N13 :  2986\n",
      "N10 :  3204\n",
      "O5 :  4374\n",
      "O23 :  4658\n",
      "N5 :  2982\n",
      "O19 :  5280\n",
      "O17 :  5085\n",
      "N17 :  3089\n",
      "O13 :  4525\n",
      "N14 :  3181\n",
      "O9 :  4686\n",
      "N11 :  3234\n",
      "O4 :  4829\n",
      "N6 :  3049\n",
      "N7 :  3521\n",
      "N19 :  3167\n",
      "N18 :  3963\n",
      "N15 :  3204\n",
      "O24 :  3747\n",
      "O1 :  5311\n",
      "O22 :  5143\n",
      "O10 :  4342\n",
      "O12 :  4380\n",
      "O3 :  4541\n",
      "O14 :  4504\n",
      "N20 :  3312\n",
      "O11 :  4750\n",
      "O18 :  4582\n",
      "O15 :  4491\n",
      "O21 :  4038\n",
      "O6 :  3813\n",
      "N21 :  3580\n",
      "O16 :  5318\n",
      "N22 :  3788\n",
      "O8 :  4491\n",
      "O20 :  4342\n",
      "O7 :  4364\n"
     ]
    }
   ],
   "source": [
    "cumulative_dfs = {}\n",
    "\n",
    "for key, value in non_terminal_paths.items():\n",
    "    merged_for_key = pd.concat([exclusive_dfs[x] for x in value], ignore_index=True)\n",
    "    cumulative_dfs.update({key: merged_for_key})\n",
    "    print(str(key), \": \", str(len(merged_for_key)))\n",
    "    #print(\"SNV Count for node: \" + str(key) + \" \" + str(len(merged_for_key)))\n",
    "\n",
    "    if write_cumulative_vcfs:\n",
    "        path_prefix = write_path_cumulative.strip(\"/\")\n",
    "        subprocess.run(['mkdir', '-p', path_prefix])\n",
    "        th_utils.write_vcf(cumulative_dfs[key], f\"{path_prefix}/{key}.vcf\", default_header)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tree_ete3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
